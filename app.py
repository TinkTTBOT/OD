import streamlit as st
from PIL import Image
import torch
import open_clip
import gc # Import th∆∞ vi·ªán Garbage Collector

# --- C·∫•u h√¨nh b·ªô nh·ªõ v√† PyTorch ---
# V√¥ hi·ªáu h√≥a t√≠nh nƒÉng d·ªçn d·∫πp b·ªô nh·ªõ t·ª± ƒë·ªông c·ªßa PyTorch ƒë·ªÉ ·ªïn ƒë·ªãnh h∆°n
torch.backends.cudnn.benchmark = False
torch.backends.cudnn.deterministic = True
gc.collect() # T·ª± d·ªçn d·∫πp b·ªô nh·ªõ ngay t·ª´ ƒë·∫ßu

# Thi·∫øt l·∫≠p c·∫•u h√¨nh trang cho chuy√™n nghi·ªáp h∆°n
st.set_page_config(
    page_title="Ph√¢n lo·∫°i Lo·∫°i Xe T·ª± ƒë·ªông b·∫±ng AI",
    page_icon="üöó",
    layout="wide"
)

# --- T·∫£i model ---
@st.cache_resource
def load_model():
    """T·∫£i m√¥ h√¨nh CLIP v√† c√°c th√†nh ph·∫ßn li√™n quan, ∆∞u ti√™n CPU ƒë·ªÉ ti·∫øt ki·ªám b·ªô nh·ªõ."""
    try:
        # T·∫£i m√¥ h√¨nh v√† g√°n r√µ r√†ng cho CPU ƒë·ªÉ tr√°nh l·ªói VRAM tr√™n c√°c m√¥i tr∆∞·ªùng b·ªã gi·ªõi h·∫°n
        device = "cpu"
        model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai', device=device)
        tokenizer = open_clip.get_tokenizer('ViT-B-32')
        return model, preprocess, tokenizer, device
    except Exception as e:
        st.error(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh: Vui l√≤ng ki·ªÉm tra k·∫øt n·ªëi m·∫°ng ho·∫∑c th∆∞ vi·ªán ƒë√£ c√†i ƒë·∫∑t. Chi ti·∫øt: {e}")
        st.stop()

model, preprocess, tokenizer, device = load_model()

# --- Danh s√°ch nh√£n (Labels) ---
labels = [
    "SUV", "Sedan", "Truck", "Van", "Bus", "Pickup Truck",
    "Hatchback", "Minivan", "Wagon", "Coupe", "Convertible"
]
# T·∫°o prompt chuy√™n nghi·ªáp h∆°n cho m√¥ h√¨nh CLIP
prompts = [f"A photo of a {label} car" for label in labels]


# ===================================================================
# --- GIAO DI·ªÜN CH√çNH (MAIN UI) ---
# ===================================================================

st.title("üöó Ph√¢n lo·∫°i Lo·∫°i Xe T·ª± ƒë·ªông b·∫±ng AI (Model CLIP)")
st.markdown("S·ª≠ d·ª•ng m√¥ h√¨nh **CLIP** ƒë·ªÉ x√°c ƒë·ªãnh lo·∫°i xe. B·∫°n c√≥ th·ªÉ nh·∫•n **Enter** sau khi t·∫£i ·∫£nh ƒë·ªÉ ph√¢n lo·∫°i.")

# T·∫°o hai c·ªôt ƒë·ªÉ b·ªë c·ª•c ƒë·∫πp h∆°n
col1, col2 = st.columns([1, 1.5]) 
image = None # Kh·ªüi t·∫°o bi·∫øn ·∫£nh
submitted = False # Kh·ªüi t·∫°o tr·∫°ng th√°i submit

# --- B·∫Øt ƒë·∫ßu Form ƒë·ªÉ k√≠ch ho·∫°t ch·ª©c nƒÉng Enter ---
with st.form("classification_form"):
    
    with col1:
        st.subheader("1. T·∫£i l√™n H√¨nh ·∫£nh Xe üì∏")
        
        # H∆∞·ªõng d·∫´n ng∆∞·ªùi d√πng c√≥ t√≠nh nƒÉng K√âO-TH·∫¢ (Drag-and-Drop)
        uploaded_file = st.file_uploader(
            "üìÅ Ch·ªçn ·∫£nh xe (.png, .jpg, .jpeg) ho·∫∑c K√©o v√† Th·∫£ v√†o ƒë√¢y:", 
            type=["png", "jpg", "jpeg"],
            key="file_uploader"
        )
        
        if uploaded_file is not None:
            try:
                image = Image.open(uploaded_file).convert("RGB")
                st.info("üí° ·∫¢nh ƒë√£ s·∫µn s√†ng. Nh·∫•n n√∫t **'B·∫Øt ƒë·∫ßu Ph√¢n lo·∫°i'** ho·∫∑c nh·∫•n ph√≠m **Enter**.")
            except Exception as e:
                st.error(f"‚ùå Kh√¥ng th·ªÉ x·ª≠ l√Ω t·ªáp ·∫£nh. L·ªói: {e}")
        else:
            st.warning("üëâ Vui l√≤ng t·∫£i l√™n m·ªôt ·∫£nh xe (ho·∫∑c k√©o th·∫£) ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

        # N√∫t ph√¢n lo·∫°i (N·∫±m trong form)
        submitted = st.form_submit_button("üîç B·∫Øt ƒë·∫ßu Ph√¢n lo·∫°i", use_container_width=True, type="primary")


# --- X·ª≠ l√Ω logic Ph√¢n lo·∫°i sau khi form ƒë∆∞·ª£c g·ª≠i (b·∫±ng n√∫t ho·∫∑c Enter) ---
if submitted:
    if image is not None:
        with col2: # Hi·ªÉn th·ªã k·∫øt qu·∫£ ·ªü c·ªôt 2
            st.subheader("2. K·∫øt qu·∫£ Ph√¢n lo·∫°i & ·∫¢nh üìä")
            st.image(image, caption="·∫¢nh ƒë√£ t·∫£i l√™n", use_container_width=True)

            with st.spinner("‚è≥ ƒêang ph√¢n t√≠ch ·∫£nh v√† t√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng..."):
                try:
                    # Ti·ªÅn x·ª≠ l√Ω ·∫£nh v√† chuy·ªÉn sang device
                    image_input = preprocess(image).unsqueeze(0).to(device)
                    text_inputs = tokenizer(prompts).to(device)
                    
                    with torch.no_grad():
                        image_features = model.encode_image(image_input)
                        text_features = model.encode_text(text_inputs)

                        # Chu·∫©n h√≥a vector
                        image_features /= image_features.norm(dim=-1, keepdim=True)
                        text_features /= text_features.norm(dim=-1, keepdim=True)

                        # T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine v√† softmax (x√°c su·∫•t)
                        probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)
                        probs = probs[0].tolist()

                        # --- X·ª≠ l√Ω K·∫øt qu·∫£ ---
                        top_idx = torch.argmax(torch.tensor(probs)).item()
                        top_label = labels[top_idx]
                        top_prob = probs[top_idx] * 100

                    st.success(f"üéâ **K·∫øt qu·∫£ Ch√≠nh:** Xe thu·ªôc lo·∫°i **{top_label}**")
                    st.metric(label="ƒê·ªô T·ª± Tin (Confidence)", value=f"{top_prob:.2f}%")

                    # Hi·ªÉn th·ªã t·∫•t c·∫£ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng bi·ªÉu ƒë·ªì
                    st.subheader("Chi ti·∫øt ƒê·ªô T∆∞∆°ng ƒê·ªìng:")
                    
                    # S·∫Øp x·∫øp k·∫øt qu·∫£ ƒë·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì tr·ª±c quan
                    results_data = sorted(zip(labels, probs), key=lambda x: x[1], reverse=True)
                    chart_labels = [r[0] for r in results_data]
                    chart_probs = [r[1] for r in results_data]
                    
                    st.bar_chart({"Lo·∫°i Xe": chart_labels, "X√°c Su·∫•t": chart_probs}, x="Lo·∫°i Xe", y="X√°c Su·∫•t")
                    
                    
                except Exception as e:
                    st.error(f"‚ùå ƒê√£ x·∫£y ra l·ªói trong qu√° tr√¨nh ph√¢n t√≠ch: {e}")
                
                finally:
                    # R·∫•t quan tr·ªçng: D·ªçn d·∫πp b·ªô nh·ªõ sau khi t√≠nh to√°n
                    if 'image_input' in locals():
                        del image_input
                    if 'text_inputs' in locals():
                        del text_inputs
                    if 'image_features' in locals():
                        del image_features
                    if 'text_features' in locals():
                        del text_features
                    if device == "cuda":
                        torch.cuda.empty_cache()
                    gc.collect() # D·ªçn d·∫πp b·ªô nh·ªõ Python
                    
    else:
        with col2:
            st.warning("‚ö†Ô∏è Vui l√≤ng t·∫£i l√™n ·∫£nh tr∆∞·ªõc khi nh·∫•n Ph√¢n lo·∫°i (ho·∫∑c Enter).")

# --- Hi·ªÉn th·ªã placeholder n·∫øu ch∆∞a c√≥ ·∫£nh v√† ch∆∞a nh·∫•n submit l·∫ßn n√†o ---
if image is None and not submitted:
    with col2:
        st.subheader("2. K·∫øt qu·∫£ Ph√¢n lo·∫°i & ·∫¢nh üìä")
        st.info("·∫¢nh xe c·ªßa b·∫°n v√† k·∫øt qu·∫£ ph√¢n lo·∫°i s·∫Ω hi·ªÉn th·ªã t·∫°i ƒë√¢y.")
        
# ===================================================================
# --- FOOTER & B·∫¢N QUY·ªÄN ---
# ===================================================================

st.markdown("---")
st.caption("""
    ¬© B·∫£n quy·ªÅn thu·ªôc v·ªÅ **Thi·ªán, C√¥ng ty AIWORKX**. 
    ·ª®ng d·ª•ng ƒë∆∞·ª£c x√¢y d·ª±ng tr√™n n·ªÅn t·∫£ng Streamlit v√† m√¥ h√¨nh th·ªã gi√°c-ng√¥n ng·ªØ **CLIP** (OpenAI/OpenCLIP).
""")
