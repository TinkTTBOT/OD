import streamlit as st
import torch
import open_clip
from PIL import Image
import io, base64

# --- C·∫•u h√¨nh trang ---
st.set_page_config(page_title="Ph√¢n lo·∫°i xe d·ª± √°n OD", page_icon="üöó", layout="centered")

# --- CSS giao di·ªán ---
st.markdown("""
    <style>
    body { background-color: #0E1117; color: white; }
    .stApp { background-color: #0E1117; }
    footer { visibility: hidden; }
    .copyright {
        text-align: center;
        font-size: 14px;
        color: #888;
        margin-top: 40px;
    }
    </style>
""", unsafe_allow_html=True)

st.title("üöó Ph√¢n lo·∫°i lo·∫°i xe d·ª± √°n OD")
st.caption("Nh·∫≠n d·∫°ng c√°c lo·∫°i xe th√¥ng d·ª•ng b·∫±ng m√¥ h√¨nh AI CLIP (OpenCLIP)")

# --- T·∫£i m√¥ h√¨nh OpenCLIP ---
@st.cache_resource
def load_model():
    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')
    tokenizer = open_clip.get_tokenizer('ViT-B-32')
    return model, preprocess, tokenizer

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess, tokenizer = load_model()
model.to(device)

# --- Danh s√°ch nh√£n ---
labels = ["SUV", "HATCHBACK", "MINIVAN", "VAN", "PICKUP TRUCK", "SEDAN", "TRUCK", "BUS", "WAGON"]

# --- L∆∞u ·∫£nh d√°n trong session_state ---
if "pasted_image" not in st.session_state:
    st.session_state.pasted_image = None

# --- Giao di·ªán upload/d√°n ·∫£nh ---
st.markdown("üñºÔ∏è **B·∫°n c√≥ th·ªÉ ch·ªçn ·∫£nh ho·∫∑c d√°n tr·ª±c ti·∫øp (Ctrl + V):**")
uploaded_file = st.file_uploader("üìÅ Ch·ªçn ·∫£nh xe", type=["jpg", "jpeg", "png"])

# --- L·∫•y ·∫£nh ---
if uploaded_file:
    image = Image.open(uploaded_file)
    st.session_state.pasted_image = image
elif st.session_state.pasted_image:
    image = st.session_state.pasted_image
else:
    st.info("üìã D√°n ·∫£nh (Ctrl + V) ho·∫∑c t·∫£i ·∫£nh ƒë·ªÉ b·∫Øt ƒë·∫ßu.")
    image = None

# --- X·ª≠ l√Ω ·∫£nh ---
if image is not None:
    st.image(image, caption="·∫¢nh xe ƒë∆∞·ª£c t·∫£i l√™n", use_column_width=True)
    image_input = preprocess(image).unsqueeze(0).to(device)
    text_tokens = tokenizer(labels).to(device)

    with torch.no_grad(), torch.cuda.amp.autocast():
        image_features = model.encode_image(image_input)
        text_features = model.encode_text(text_tokens)
        image_features /= image_features.norm(dim=-1, keepdim=True)
        text_features /= text_features.norm(dim=-1, keepdim=True)
        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)
        probs = similarity[0].cpu().numpy()

    st.success("‚úÖ K·∫øt qu·∫£ ph√¢n lo·∫°i:")
    for label, prob in zip(labels, probs):
        st.write(f"**{label}**: {prob * 100:.2f}%")

# --- Script h·ªó tr·ª£ d√°n ·∫£nh (l∆∞u v√†o session_state, kh√¥ng reload) ---
st.markdown("""
<script>
document.addEventListener('paste', async (event) => {
    const items = event.clipboardData.items;
    for (const item of items) {
        if (item.type.indexOf('image') !== -1) {
            const blob = item.getAsFile();
            const reader = new FileReader();
            reader.onload = function(e) {
                const base64Image = e.target.result;
                // G·ª≠i base64 l√™n Streamlit qua streamlit.setComponentValue
                const input = document.createElement('input');
                input.type = 'text';
                input.id = 'pasted_image_input';
                input.value = base64Image;
                input.style.display = 'none';
                document.body.appendChild(input);
                input.dispatchEvent(new Event('change'));
            };
            reader.readAsDataURL(blob);
        }
    }
});
</script>
""", unsafe_allow_html=True)

# --- B·∫£n quy·ªÅn ---
st.markdown("""
<div class="copyright">¬© B·∫£n quy·ªÅn b·ªüi <b>Dino (Thien)</b> ‚Äì C√¥ng ty <b>AIWORX</b></div>
""", unsafe_allow_html=True)
